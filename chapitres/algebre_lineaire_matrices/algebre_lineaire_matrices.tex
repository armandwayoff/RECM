\chapter{Algèbre linéaire, matrices}
\labch{algebre_lineaire_matrices}

\textsl{Au \textsc{xviii}$^\me$ siècle se développent la résolution des systèmes linéaires et la théorie des déterminants. Les raisonnements suggèrent rapidement le concept d'espace à $n$ dimensions. Mais il fallait oser un langage géométrique, alors qu'une interprétation sensible dans le plan ou l'espace faisait défaut pour $n > 3$. \\
De manière indépendante, \textsc{Cayley} en Angleterre et \textsc{Grassman} en Allemagne franchissent le pas vers 1843-1845 et parlent d'espace à $n$ dimensions. Le point de vue de \textsc{Cayley} est issu directement de la géométrie analytique: un vecteur d'un espace à $n$ dimensions est un système de $n$ réels ou $n$ complexes. L'addition de deux vecteurs et la multiplication par un scalaire sont naturellement introduites par la généralisation de la dimension $3$. Pour parvenir vraiment à la notion d'espace vectoriel, il faut dégager le concept de sous-espace et de dimension d'un sous-espace. C'est ce que fera \textsc{Grassman} (professeur de lycée autodidacte en marge des milieux de la recherche) en cherchant à développer une analyse géométrique portant sur des calculs intrinsèques indépendants du choix des coordonnées. \textsc{Grassman} introduit le produit extérieur de deux vecteurs, la définition de l'indépendance linéaire, de la dimension d'un espace et démontre la relation fondementale
$$\dim V + \dim W = \dim (V + W) + \dim V \cap W.$$
Ces travaux eurent peu d'impact au début, mais ils furent repris par Henri \textsc{Poincaré} et Élie \textsc{Cartan} (notamment son \say{algèbre extérieure} en géométrie différentielle). \\
C'est en 1888 que \textsc{Peano} donnera la définition axiomatique d'un espace vectoriel réel. Jusqu'en 1930, le point de vue des matrices et des coordonnées prédomine par rapport au point de vue intrinsèque des espaces vectoriels.
}

\begin{marginfigure}[-13cm]
    \centering
    \includegraphics{images/arthur_cayley.png}
    \caption*{\centering Arthur \textsc{Cayley} (1821-1895)}
\end{marginfigure}

\begin{marginfigure}[-6cm]
    \centering
    \includegraphics{images/hermann_grassmann.png}
    \caption*{\centering Hermann \textsc{Grassmann} (1809-1877)}
\end{marginfigure}

\newpage

% \section{Cours: changement de base}
% \begin{marginfigure}
%     \input{illustrations/i_changement_de_base}
% \end{marginfigure}

\section{Produit d'endomorphismes nilpotents qui commutent}
\input{chapitres/algebre_lineaire_matrices/produit_endomorphismes_nilpotents_qui_commutent}

\section{Centre de \texorpdfstring{$\M_n(\K)$}{l'espace des matrices carrées}}
\input{chapitres/algebre_lineaire_matrices/centre_de_l_espace_des_matrices_carres}

\section{Semblables sur \texorpdfstring{$\C$, sur $\R$}{C, sur R}}
\input{chapitres/algebre_lineaire_matrices/semblables_sur_C_sur_R}

\section{Noyaux itérés}
\input{chapitres/algebre_lineaire_matrices/noyaux_iteres}

\section{Applications de \texorpdfstring{$\M_n(\K) \to \K$}{l'espace des matrices carrées dans le corps K} conservant le produit}
\input{chapitres/algebre_lineaire_matrices/applications_de_l_espace_des_matrices_carrees_dans_le_corps_K_conservant_le_produit}

\section{Matrices compagnon et commutant d'un cyclique}
\input{chapitres/algebre_lineaire_matrices/matrices_compagnon_et_commutant_d_un_cyclique}

\section{Caractérisation des homothéties}
\input{chapitres/algebre_lineaire_matrices/caracterisation_des_homotheties}

\section{Polynômes de \textsc{Hilbert}} \label{polynome_hilbert}
\input{chapitres/algebre_lineaire_matrices/polynomes_de_hilbert}

\section{Polynômes de \textsc{Lagrange}} 
\input{chapitres/algebre_lineaire_matrices/polynomes_de_lagrange}

\section{Polynômes d'interpolation de \textsc{Lagrange}, lien avec les déterminants de \textsc{Vandermonde}}
\input{chapitres/algebre_lineaire_matrices/polynomes_interpolation_de_lagrange_lien_avec_les_determinants_de_vandermonde}

\section{Indice de nilpotence en dimension finie} \label{indice_nilpotence}
\input{chapitres/algebre_lineaire_matrices/indice_de_nilpotence_en_dimension_finie}

\section{Matrices de taille $3$ d'ordre de nilpotence égal à $2$}
\input{chapitres/algebre_lineaire_matrices/matrices_de_taille_trois_odre_de_nilpotence_deux}

\section{Famille libre engendrée par un endomorphisme nilpotent}
\input{chapitres/algebre_lineaire_matrices/famille_libre_engendree_par_un_endomorphisme_nilpotent}

\section{Matrices de rang $1$}
\input{chapitres/algebre_lineaire_matrices/matrices_de_rang_un}

\section{Sous-espace engendré par les matrices nilpotentes}
\input{chapitres/algebre_lineaire_matrices/sous_espace_engendre_par_les_matrices_nilpotentes}

\section{Titre à compléter}
\begin{exercice}
    \cite{acamanes} 
    Soient $A$ et $B$ deux matrices de $\M_n(\C)$ qui commutent. 
    \begin{enumerate}
        \item On suppose que $B$ est nilpotente. 
        \begin{enumerate}
            \item Montrer que $A + B$ est inversible si et seulement si $A$ est inversible.
            \item Montrer que $\det(A+B) = \det(A)$.
        \end{enumerate}
        \item Soient $A_1, \dots, A_n$ des matrices nilpotentes qui commutent deux à deux. Montrer que $A_1 \times \cdots \times A_n = 0$.
    \end{enumerate}
\end{exercice}


\section{Si $AB - BA = A \dots$}
\textcolor{red}{textorpdfstring}
\begin{exercice}
    fic00118 [005625]\\
    Soit $(A, B) \in \M_n(\R)^2$ tel ques $AB-BA=A$. Montrer que pour tout $p \in \Ne$, $\Tr(A^p) = 0$.
\end{exercice}

\marginnote[2cm]{
    \begin{kaobox}[frametitle=Propriétés de la trace]
        Soit $(A, B) \in \M_n(\K)^2$.
        $$\Tr(AB) = \Tr(BA),$$
        $$\Tr(A + \lambda B) = \Tr(A) + \lambda \Tr(B).$$
    \end{kaobox}
}

Ma solution est beaucoup simple que la leur...
\begin{solution}
    Soit $p \in \Ne$. On écrit
    \begin{align*}
        A^p = A^{p-1}(AB-BA) = A^pB - A^{p-1}BA.
    \end{align*}
    On composant cette relation par la trace on obtient d'après ses propriétés
    \begin{align*}
        \Tr(A^p) &= \Tr(A^pB) - \Tr((A^{p-1}B)A) \\
        &= \Tr(A^pB) - \Tr(A(A^{p-1}B)) \\
        \Tr(A^p) &= 0.
    \end{align*}
\end{solution}

\begin{exercice}
    \cite{exos_oraux} p. 47 \\
    Soient $n \in \Ne$, $A$ et $B$ deux matrices de $\M_n(\R)$ telles que $AB - BA = A.$
    \begin{enumerate}
        \item Montrer que la matrice $A$ n'est pas inversible.
        \item Montrer que pour tout $k \in \Ne$, $AB^k - B^k A = k A^k$. En déduire que la matrice $B$ est nilpotente. 
    \end{enumerate}
\end{exercice}

\begin{remarque}
    On retrouve le résultat (à vérifier et à justifier) qu'une matrice $A$ est nilpotente si et seulement si pour tout $k \in \Ne, \Tr(A^k) = 0$.
\end{remarque}


\section{Application du théorème de recollement}

\begin{exercice}
    \cite{acamanes}
    Soient $E$ et $F$ deux espaces vectoriels de dimension finie et $f \in \Endo(E, F)$. On note $\mathscr{H} \defeq \{ g \in \Endo(F, E);\ f \circ g \circ f = 0\}$. Déterminer la dimension de $\mathscr{H}$ en fonction de $\dim E$, $\dim F$ et $\Rg f$.
\end{exercice}

\marginnote[0cm]{
    \cite{acamanes} Ch3, Th2
    \begin{kaobox}[frametitle=Sommes directe \& Applications linéaires]
        On suppose que $E = \bigoplus\limits_{i=1}^m E_i$. Pour tout indice $i \in \llbracket 1, m \rrbracket$, on considère une application linéaire $\varphi_i$ de $E_i$ dans $F$. Alors, il existe une unique application linéaire $\varphi$ de $E$ dans $F$ telle que pour tout $i \in \llbracket 1, m \rrbracket$, la restriction de $\varphi$ à $E_i$ soit égale à $\varphi$.
    \end{kaobox}
}

\begin{solution}
    \underline{Première solution:}\\
    \begin{itemize}
        \item On montre que $\mathscr{H}$ est un sous-espace vectoriel de $\Endo(F, E)$. Soit $W$ un supplémentaire de $\Im f$ dans $F$.
        \item On va construire un isomorphisme entre $\mathscr{H}$ et un ensemble dont on peut calculer la dimension. On pose
        \begin{alignat*}{2}
            \psi\ :\ \mathscr{H}\ &\longrightarrow\ \Endo(\Im f, \Ker f) \times \Endo(W, E)\\
            g\ &\longmapsto\ (g_{\vert \Im f}, g_{\vert \Ker f}).
        \end{alignat*}
        Montrons que $\psi$ est un isomorphisme. 
        \begin{itemize}
            \item Montrons que $\psi$ est bien définie: \\
            soient $x \in E$ et $g \in \mathscr{H}$,
            $$\psi(g)(x) = \left( g_{\vert \Im f}(x), g_{\vert W}(x) \right).$$
            De plus, $\Im g \circ f \subset \Ker f$ donc $g_{\vert \Im f} \in \Endo(\Im f, \Ker f)$.
            $$(f \circ g \circ f)(x) = f(g(\underbrace{f(x)}_{\defeq y})) = f(\underbrace{g_{\vert \Im f}(y)}_{\in \Ker f}) = 0.$$
            La linéarité est triviale. 
            \item Montrons que $\psi$ est surjective. \\
            Soit $(g_1, g_2) \in \Endo(\Im f, \Ker f) \times \Endo(W, E)$. \\
            Par théorème de recollement des applications linéaires, il existe une application $g \in \Endo(F, E)$ telle que 
            \begin{cases}
                    g_{\vert \Im f} = g_1 \\
                    g_{\vert W} = g_2
            \end{cases}. \\
            Soit $x \in E$, $(f \circ g \circ f)(x) = f(g(\underbrace{f(x)}_{\in \Im f}) = f(\underbrace{g_1(f(x))}_{\in \Ker f}) = 0$ donc $g \in \mathscr{H}$ et $\psi$ est sujective. 
            \item Montrons que $\psi$ est injective. \\
            Soit $g \in \Ker \psi$. Alors $(g_{\vert \Im f}, g_{\vert W}) = (0_{\Endo(F, E)}, 0_{\Endo(F, E)})$. \\
            Donc $g = 0_{\Endo(F, E)}$ et $\Ker \psi = \{ 0_{\Endo(F, E)} \}$ soit $\psi$ est injective. \\
            Finalement, $\psi$ est un isomorphisme. \\
            Il y a donc égalité des dimensions entre les espaces de départ et d'arrivée de $\psi$. Ainsi,
            \begin{align*}
                \dim \mathscr{H} &= \dim(\Endo(\Im f, \Ker f)) + \dim (\Endo(W, E)) \\
                &= \Rg f \times \dim \Ker f + \dim W \times \dim E \\
                &= \Rg f \times (\dim E - \Rg f) + (\dim F - \Rg f) \times \dim E \\
                \dim \mathscr{H} &= \dim E \dim F - (\Rg f)^2
            \end{align*}
        \end{itemize}
        \begin{remarque}
            Si $\dim E = \dim F$ et que $f$ est bijective, le résultat est cohérent. De même si $f = 0_{\Endo(E, F)}$.
        \end{remarque}
    \end{itemize}
    \underline{Une solution plus visuelle:} \\
    Soit $W$ un supplémentaire de $\Im f$ dans $F$: $\Im f \oplus W = F$ et soit $V$ un supplémentaire de $\Ker f$ dans $E$: $\Ker f \oplus V = E$. \\
    Soit $\mathscr{B} \defeq (e_1, \dots, e_r, e_{r+1}, \dots, e_p)$ une base adaptée à la décomposition $\Im f \oplus W = F$ et soit $\mathscr{B}' \defeq (\varepsilon_1, \dots, \varepsilon_{n-r}, \varepsilon_{n-r+1}, \dots, \varepsilon_n)$ une base adaptée à la décomposition $\Ker f \oplus V = E$.
    \begin{align*}
        g \in \mathscr{H} &\Leftrightarrow g(\Im f) \subset \Ker f \\
        &\Leftrightarrow \Mat_{\mathscr{B}, \mathscr{B}'}(g) = 
        \begin{pmatrix}
            \star & \star \\
            0 & \star
        \end{pmatrix}
    \end{align*}
\end{solution}