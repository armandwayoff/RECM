\begin{defi}{Matrice de \textsc{Gram}}
    Soient $E$ un espace euclidien et $(x_1, \dots, x_p) \in E^n$. On définit la matrice de \textsc{Gram} par
    $$\Gram(x_1, \dots, x_p) \defeq \big( \langle x_i, x_j \rangle \big)_{i,j \in \llbracket 1, p \rrbracket}.$$
\end{defi}
Le déterminant de \textsc{Gram} permet de calculer des volumes et de tester l'indépendance linéaire d'une famille de vecteurs.
\begin{prop}{}
    $$\Rg \big( \Gram(x_1, \dots, x_n) \big) = \Rg(x_1, \dots, x_n)$$
\end{prop}

\begin{preuve}
    \marginnote[0cm]{Source : \cite{maths-france} Planche 6 Maths SPE}
    Soient $F \defeq \Vect(x_1, \dots, x_n)$ et $m \defeq \dim F$. Soient $\mathscr{B} \defeq (e_i)_{1 \leqslant i \leqslant m}$ une base orthonormée de $F$ puis $M$ la matrice de la famille $(x_j)_{1 \leqslant j \leqslant n}$ dans la base $\mathscr{B}$. La matrice $M$ est une matrice rectangulaire de format $(m, n)$. \\
    Soit $(i, j) \in \llbracket 1, m \rrbracket \times \llbracket 1, n \rrbracket$. Puisque la base $\mathscr{B}$ est orthonormée, le coefficient $[ \Trsp{M} M ]_{i,j}$ est 
    $$\sum_{k=1}^m m_{k,i} m_{k,j} = \langle x_i, x_j \rangle,$$
    et on a donc
    $$\Gram(x_1, \dots, x_n) = \Trsp{M} M.$$
    Puisque $\Rg(x_1, \dots, x_n) = \Rg M$, il s'agit de vérifier que $\Rg(\Trsp{M} M) = \Rg M$. Pour cela, montrons que les matrices $M$ et $\Trsp{M} M$ ont le même noyau. \\
    Soit $X \in \M_{n,1}(\R)$.
    \begin{align*}
        X \in \Ker M &\implies MX = 0 \\
        &\implies \Trsp{M}MX = 0 \\
        &\implies X \in \Ker(\Trsp{M}M)
    \end{align*}
    et aussi
    \begin{align*}
        X \in \Ker \big( \Trsp{M}M \big) &\implies \Trsp{M}MX = 0 \\
        &\implies \Trsp{X}\Trsp{M}MX = 0 \\
        &\implies \Trsp{(MX)}MX = 0 \\
        &\implies \norme{MX}^2 = 0 \\
        &\implies MX = 0 \\
        &\implies X \in \Ker M.
    \end{align*}
    Finalement, $\Ker(\Trsp{M}M) = \Ker M$ et donc, d'après le théorème du rang,
    $$\Rg(x_1, \dots, x_n) = \Rg M = \Rg(\Trsp{M}M) = \Rg \big( \Gram(x_1, \dots, x_n) \big).$$
\end{preuve}

\begin{prop}{}
     La famille $(x_1, \dots, x_p)$ est liée si et seulement si $\det \Gram(x_1, \dots, x_p) = 0$ et est libre si et seulement si $\det \Gram(x_1, \dots, x_p) > 0$.
\end{prop}

\begin{preuve}
    \marginnote[0cm]{Source : \cite{maths-france} Planche 6 Maths SPE}
    D'après (??) et puisque $\Trsp{M}M \in \M_n(\K)$,
    \begin{align*}
        (x_1, \dots, x_n) \text{ liée } &\iff \Rg (x_1, \dots, x_n) < n \\
        &\iff \Rg \Gram(x_1, \dots, x_n) < n \\
        &\iff \Gram(x_1, \dots, x_n) \not \in \Gl_n(\R) \\
        &\iff \det \Gram(x_1, \dots, x_n) = 0.
    \end{align*}
    De plus, quand la famille $(x_1, \dots, x_n)$ libre, avec les notations de (??), on a $m=n$ et la matrice $M$ est une matrice carrée, inversible. On peut donc écrire
    $$\det \Gram(x_1, \dots, x_n) = \det \big( \Trsp{M} M \big) = \det(M)^2 > 0.$$
\end{preuve}

\begin{theo}{Distance à un sous-espace vectoriel} \labthm{distance_a_un_sous_espace_vectoriel}
    Soit $E$ un espace préhilbertien. Soit $F$ un sous-espace vectoriel de $E$ de dimension finie $p \in \Ne$ et soit $(e_1, \dots, e_p)$ une base de $F$. Alors pour tout $x \in E$,
    $$d(x, F)^2 = \frac{\Gram(e_1, \dots, e_p, x)}{\Gram(e_1, \dots, e_p)}.$$
\end{theo}

\begin{preuve}
    Soit $\pi_F(x)$ le projeté orthogonal de $x$ sur $F$. \\
    Alors $d(x, F)^2 = \norme{x - \pi_F(x)}^2$ et par \textsc{Pythagore},
    $$\norme{x}^2 = \norme{\pi_F(x)}^2 + \norme{x - \pi_F(x)}^2.$$
    De plus, 
    $$\forall k \in \llbracket 1, p \rrbracket,\ \langle x , e_k \rangle = \langle \pi_F(x) , e_k \rangle.$$
    On obtient alors
    \begin{align*}
        \Gram(e_1, \dots, e_p, x) &= 
        \begin{vmatrix}
          \begin{matrix}
            & & \\
            & \langle e_i, e_j \rangle & \\
            & &
          \end{matrix}
          & \rvline & \langle e_i, x \rangle \\
        \hline
          \langle x, e_j \rangle & \rvline &
          \begin{matrix}
          \norme{x}^2
          \end{matrix}
        \end{vmatrix} \\
        &=
        \begin{vmatrix}
          \begin{matrix}
            & & \\
            & \langle e_i, e_j \rangle & \\
            & &
          \end{matrix}
          & \rvline & \langle e_i, \pi_F(x) \rangle + 0 \\
        \hline
          \langle x, e_j \rangle & \rvline &
          \begin{matrix}
          \norme{\pi_F(x)}^2 + \norme{x - \pi_F(x)}^2
          \end{matrix}
        \end{vmatrix} 
    \end{align*}
    \marginnote[0cm]{
        \note On écrit la dernière colonne sous la forme
        $$
        \begin{pmatrix}
            \langle e_1, \pi_F(x) \rangle \\
            \vdots \\
            \langle e_p, \pi_F(x) \rangle \\
            \norme{\pi_F(x)}^2
        \end{pmatrix}
        + 
        \begin{pmatrix}
            0 \\
            \vdots \\
            0 \\
            \norme{x - \pi_F(x)}^2
        \end{pmatrix}.
        $$
    }
    Par linéarité du déterminant par rapport à la dernière colonne \note on obtient
    $$\Gram(e_1, \dots, e_p, x) = \Gram \big(e_1, \dots, e_p, \pi_F(x) \big) + \norme{x - \pi_F(x)}^2 \Gram(e_1, \dots, e_p).$$
    Comme $\pi_F(x) \in \Vect(e_1, \dots, e_p)$, le premier terme est nul et donc 
    $$d(x, F)^2 = \frac{\Gram(e_1, \dots, e_p, x)}{\Gram(e_1, \dots, e_p)}.$$
\end{preuve}

\begin{corol} \labthm{inegalite_gram}
    Soit $(x_1, \dots, x_n) \in E^n$. Alors,
    $$\Gram(x_1, \dots, x_n) \leqslant \prod_{i=1}^n \norme{x_i}^2$$
    avec égalité si et seulement si la famille $(x_1, \dots, x_n)$ est orthogonale. 
\end{corol}

Compléter avec \cite{objectif_agregation} p. 185.

\begin{preuve}
    \marginnote[0cm]{Source : \href{http://vonbuhren.free.fr/Agregation/Developpements/dev_determinant_gram.pdf}{Développement : Déterminant de \textsc{Gram} -- Jérôme \textsc{Von Buhren}, \textsf{vonbuhren.free.fr}}}
    \begin{itemize}
        \item Si la famille $(x_1, \dots, x_n)$ est liée, le résultat est immédiat.
        \item Raisonnons par récurrence sur $n \in \Ne$ sur la propriété
        \begin{center}
            $\mathscr{P}_n$: \say{ pour toute famille libre $(x_1, \dots, x_n)$ de $E$, on a $\Gram(x_1, \dots, x_n) \leqslant \prod\limits_{i=1}^n \norme{x_i}^2$ }.
        \end{center}
        \begin{itemize}
            \item[$\rhd$] Initialisation pour $n = 1$: soit $x_1 \in E$. Par définition, $\Gram(x_1) = \langle x_1, x_1 \rangle = \norme{x_1}^2$ donc $\mathscr{P}_1$ est vérifiée.
            \item[$\rhd$] Hérédité: supponsons $\mathscr{P}_n$ vraie. Soit $(x_1, \dots, x_n, x_{n+1})$ une famille libre de $E$. En notant $F \defeq \Vect(x_1, \dots, x_n)$, il existe $(f, \pi_F) \in F \times F^\perp$ tel que $x_{n+1} = f + \pi_F$. Par le théorème \vrefthm{distance_a_un_sous_espace_vectoriel} et par $\mathscr{P}_n$, 
            $$\Gram(x_1, \dots, x_{n+1}) = \Gram(x_1, \dots, x_n) \norme{\pi_F} \leqslant \norme{x_1}^2 \cdots \norme{x_n}^2 \norme{x_{n+1}}^2$$
            car par le théorème de \textsc{Pythagore}, $\norme{\pi_F} \leqslant \norme{x_{n+1}}$. On conclut que $\mathscr{P}_{n+1}$ est vraie, d'où le résultat. 
        \end{itemize}
    \end{itemize}
    \textcolor{red}{cas d'égalité}
\end{preuve}

\begin{prop}{}
    La matrice de \textsc{Gram} est symétrique positive.
\end{prop}

\marginnote[0cm]{
    \begin{defi}{Matrices symétriques positives}
        L'ensemble des \emph{matrices symétriques positives} est noté $\mathscr{S}_n^+(\R)$. Une matrice $M \in \mathscr{S}_n^+(\R)$ équivaut à chacune des propriétés suivantes:
        \begin{itemize}
            \item pour tout $X \in \M_n(\R), \Trsp{X} M X \geqslant 0$,
            \item $\Sp(M) \subset \Rp.$
        \end{itemize}
    \end{defi}
}

\begin{preuve}
    \begin{itemize}
        \item La matrice de \textsc{Gram} est symétrique par symétrie du produit scalaire.
        \item Montrons la positivité de $\Gram$. Soit $X = \Trsp{(\alpha_1 \cdots \alpha_n)} \in \M_{n,1}(\R)$. Montrons que $\Trsp{X} \Gram X \geqslant 0$. 
        \begin{align*}
            \Trsp{X} \Gram X &= \sum_{i=1}^{n} \sum_{j=1}^{n} \langle x_i, x_j \rangle \alpha_i \alpha_j \\ 
            &= \sum_{i=1}^{n} \sum_{j=1}^{n} \langle \alpha_i x_i, \alpha_j x_j \rangle \\
            &= \left\Vert \sum_{i=1}^{n}x_i \alpha_i \right\Vert^2 \geqslant 0.
        \end{align*}
    \end{itemize}
   
    Ce qui montre bien que $\Gram$ est symétrique positive.
\end{preuve}
