\begin{defi}
    Soit $E$ un espace euclidien et $(x_1, \dots, x_p)$ une famille d'éléments de $E$. On définit
    $$\text{la matrice de \textsc{Gram} } \Gram \defeq \left( \langle x_i, x_j \rangle \right)_{i,j \in \llbracket 1, p \rrbracket}$$
    $$\text{le déterminant de \textsc{Gram} } \Gram(x_1, \dots, x_p) \defeq \det \Gram.$$
\end{defi}

\marginnote{Le déterminant de \textsc{Gram} permet de calculer des volumes et de tester l'indépendance linéaire d'une famille de vecteurs.}

\begin{remarque}
    La matrice $\Gram$ est symétrique.
\end{remarque}

\begin{prop}
     La famille $(x_1, \dots, x_p)$ est liée si et seulement si $\Gram(x_1, \dots, x_p) = 0$.
\end{prop}

\begin{preuve}
    $(\Rightarrow)$ Il existe une famille $(\lambda_1, \dots, \lambda_p)$ non nulle  de $\R^n$ telle que $\sum\limits_{i=1}^{p} \lambda_i x_i = 0$. On montre alors que pour toute ligne $L_i$ de $\Gram$, $\sum\limits_{i=1}^{p} \lambda_i L_i = 0$ ce qui permet de conclure. \\
    $(\Leftarrow)$ Raisonner par contraposée, on suppose $\mathscr{F} = (x_1, \dots, x_p)$ libre. \\
    Soit $\mathscr{B} = (\varepsilon_1, \dots, \varepsilon_n)$ une b.o.n. de $E$ et $H \in \M_{n, p} (\R)$ la matrice de $\mathscr{F}$ dans $\mathscr{B}$. \\
    Alors $\boxed{\Trsp{H} H = \Gram(x_1, \dots, x_p)}$. \\
    Montrons la chaîne :
    $$\Rg(G) = \underbrace{\Rg(\Trsp{H} H) = \Rg(H)}_{\text{à montrer}} = \Rg(\mathscr{F}) = p \not= 0$$
    Montrer que $\Rg(\Trsp{H} H) = \Rg(H)$ en montrant que $\Ker(\Trsp{H} H) = \Ker(H)$. 
    \begin{itemize}
        \item $(\subset)$ oui
        \item $(\supset)$ Soit $X \in \Ker(\Trsp{H}H)$. On montre que $\norme{BX} = 0 \Rightarrow BX = 0$. 
    \end{itemize}
    Par le \textbf{théorème du rang}, on obtient le résultat. 
\end{preuve}

\begin{prop}
    Distance à un sous-espace vectoriel \\
    Soit $(E, \langle \cdot , \cdot \rangle)$ un espace préhilbertien. \\
    Soit $F$ un sous-espace vectoriel de $E$ de dimension finie $p \in \Ne$ et soit $(e_1, \dots, e_p)$ une base de $F$. Alors pour tout $x \in E$,
    $$d(x, F)^2 = \frac{\Gram(e_1, \dots, e_p, x)}{\Gram(e_1, \dots, e_p)}.$$
\end{prop}

\begin{preuve}
    Inspirée par celle de Florian DUSSAP. \\
    Soit $\pi_F(x)$ le projeté orthogonal de $x$ sur $F$. \\
    Alors $d(x, F)^2 = \norme{x - \pi_F(x)}^2$ et par \textsc{Pythagore}, $\norme{x}^2 = \norme{\pi_F(x)}^2 + \norme{x - \pi_F(x)}^2$. De plus, pour tout $k \in \llbracket 1, p \rrbracket$, $\langle x | e_k \rangle = \langle \pi_F(x) | e_k \rangle$.
    \begin{align*}
        \Gram(e_1, \dots, e_p, x) &= 
        \begin{vmatrix}
          \begin{matrix}
            & & \\
            & \langle e_i, e_j \rangle & \\
            & &
          \end{matrix}
          & \rvline & \langle e_i, x \rangle \\
        \hline
          \langle x, e_j \rangle & \rvline &
          \begin{matrix}
          \norme{x}^2
          \end{matrix}
        \end{vmatrix} \\
        & = \begin{vmatrix}
          \begin{matrix}
            & & \\
            & \langle e_i, e_j \rangle & \\
            & &
          \end{matrix}
          & \rvline & \langle e_i, \pi_F(x) \rangle \\
        \hline
          \langle \pi_F(x), e_j \rangle & \rvline &
          \begin{matrix}
          \norme{\pi_F(x)}^2
          \end{matrix}
        \end{vmatrix}
        + 
        \begin{vmatrix}
          \begin{matrix}
            & & \\
            & \langle e_i, e_j \rangle & \\
            & &
          \end{matrix}
          & \rvline & 0 \\
        \hline
          0 & \rvline &
          \begin{matrix}
          \norme{x - \pi_F(x)}^2
          \end{matrix}
        \end{vmatrix} \\
        &= \Gram(e_1, \dots, e_p, \pi_F(x)) + \norme{x - \pi_F(x)}^2 \Gram(e_1, \dots, e_p).
    \end{align*}
    Comme $\pi_F(x) \in \Vect(e_1, \dots, e_p), \Gram(e_1, \dots, e_p, \pi_F(x)) = 0$ et donc $\Gram(e_1, \dots, e_p, x) = d(x, F)^2 \Gram(e_1, \dots, e_p)$.
\end{preuve}

Corollaire.

\begin{tcolorbox}
    Soit $(x_1, \dots, x_n) \in E^n$. Alors,
    $$\Gram(x_1, \dots, x_n) \leqslant \prod_{i=1}^n \norme{x_i}^2$$
    avec égalité si et seulement si la famille $(x_1, \dots, x_n)$ est liée. 
\end{tcolorbox}

\begin{preuve}
    
\end{preuve}

\begin{tcolorbox}
    La matrice de \textsc{Gram} est symétrique positive.
\end{tcolorbox}

\begin{preuve}
    \begin{itemize}
        \item La matrice de \textsc{Gram} est symétrique par symétrie du produit scalaire.
        \item Montrons la positivité de $\Gram$. Soit $X = \Trsp{(\alpha_1 \cdots \alpha_n)} \in \M_{n,1}(\R)$. Montrons que $\Trsp{X} \Gram X \geqslant 0$. 
        \begin{align*}
            \Trsp{X} \Gram X &= \sum_{i=1}^{n} \sum_{j=1}^{n} \langle x_i, x_j \rangle \alpha_i \alpha_j \\ 
            &= \sum_{i=1}^{n} \sum_{j=1}^{n} \langle \alpha_i x_i, \alpha_j x_j \rangle \\
            &= \left\Vert \sum_{i=1}^{n}x_i \alpha_i \right\Vert^2 \geqslant 0.
        \end{align*}
    \end{itemize}
   
    Ce qui montre bien que $\Gram$ est symétrique positive.
\end{preuve}

\marginnote[-10cm]{
    \begin{kaobox}[frametitle=Matrices symétriques positives]
        L'ensemble des \emph{matrices symétriques positives} est noté $\mathscr{S}_n^+(\R)$. Une matrice $M \in \mathscr{S}_n^+(\R)$ équivaut à chacune des propriétés suivantes:
        \begin{itemize}
            \item pour tout $X \in \M_n(\R), \Trsp{X} M X \geqslant 0$,
            \item $\Sp(M) \subset \Rp.$
        \end{itemize}
    \end{kaobox}
}
